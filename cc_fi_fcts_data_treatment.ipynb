{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701643e4",
   "metadata": {},
   "source": [
    "### Objective\n",
    "### This file cotains definitions of  employed functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2d9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.linalg import svd\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import sklearn.cluster as cluster\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc07914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct ploting feature variables of a dataframe\n",
    "\n",
    "def fct_show_raw_visualization(\n",
    "    data,\n",
    "    feature_keys,\n",
    "    titles,\n",
    "    colors,\n",
    "    v_cols,\n",
    "    v_name_dataframe):\n",
    "    \n",
    "    #print(\"data.shape[0]\",data.shape[0])\n",
    "    #print()\n",
    "    #print(\"range(len(feature_keys))\",range(len(feature_keys)))\n",
    "    #print()\n",
    "    v_nb_rows=math.ceil(len(feature_keys)/v_cols)\n",
    "    \n",
    "    \n",
    "    #time_data = data[date_time_key]\n",
    "    x_axis_labels=list(range(0,data.shape[0]))\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=v_nb_rows, ncols=v_cols, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    \n",
    "    #title \n",
    "    fig.suptitle('Features dataframe '+str(v_name_dataframe))\n",
    "    \n",
    "    # Remove the subplot at position (9,2)which is element (8,1)\n",
    "    #as numeration starts from zero\n",
    "    if len(feature_keys)<v_nb_rows*v_cols:\n",
    "        row_id=int(v_nb_rows-1)\n",
    "        fig.delaxes(axes[row_id,1])\n",
    "    \n",
    "    \n",
    "    for i in range(len(feature_keys)):\n",
    "        #print(\"i\",i)\n",
    "    \n",
    "        key = feature_keys[i]\n",
    "        \n",
    "        #print(\"key\",key)\n",
    "        c = colors[i]\n",
    "        t_data = data[key]\n",
    "        \n",
    "        #print(\"t_data\",t_data)\n",
    "       \n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i//2, i%2],\n",
    "            color=c,\n",
    "            title=\"{} - {}\".format(titles[i], key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9caf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d5afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function creating a heatmap plot from a dataframe\n",
    "\n",
    "def fct_show_feature_heatmap(data,val_name_dataframe):\n",
    "    plt.matshow(data.corr())\n",
    "    plt.xticks(range(data.shape[1]), data.columns, fontsize=12, rotation=90,color=\"b\")\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.yticks(range(data.shape[1]), data.columns, fontsize=12,color=\"b\")\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=12)\n",
    "    plt.title(\"Feature Correlation Heatmap-\"+str(val_name_dataframe), fontsize=10,color=\"b\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8e80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b12da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct creating a correlation coefficient matrix from a dataframe\n",
    "def fct_show_cor_coef_matrix(val_data,val_name_dataframe):\n",
    "    \n",
    "    plt.figure(figsize=(14,7))\n",
    "    #plt.figure(figsize=(20,15))\n",
    "\n",
    "    # Create a custom divergin palette\n",
    "    cmap = sns.diverging_palette(100, 7, s=75, l=40,n=5, center=\"light\", as_cmap=True)\n",
    "    \n",
    "    #Create a mask\n",
    "    mask = np.triu(np.ones_like(val_data.corr(), dtype=bool))\n",
    "    np.fill_diagonal(mask, False)\n",
    "    \n",
    "    #scale all fonts in your legend and on the axes.\n",
    "    sns.set(font_scale=1)\n",
    "    \n",
    "    heatmap = sns.heatmap(val_data.corr(), vmin=-1, vmax=1, annot=True,\\\n",
    "                          annot_kws={'fontsize': 8},cmap=\"PiYG\", mask=mask)\n",
    "    \n",
    "    heatmap.set_title('Correlation coeffcient matrix-'+str(val_name_dataframe),\\\n",
    "                  fontdict={'fontsize':15}, pad=12,c=\"darkgreen\")\n",
    "    \n",
    "    plt.yticks(rotation=30,fontsize=8,c=\"darkgreen\")\n",
    "    plt.xticks(rotation=20,fontsize=8,c=\"darkgreen\")\n",
    "    \n",
    "    #plt.savefig(\"Figures/fig_cor_heatmap.png\")\n",
    "    #plt.close(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db5c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e22ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct standardizing the dataframe val_df\n",
    "def fct_stand_dataframe(val_df):\n",
    "    \n",
    "    v_mean=val_df.mean(axis=0)\n",
    "    v_std=val_df.std(axis=0)\n",
    "    \n",
    "    df_stand=(val_df-v_mean)/v_std\n",
    "    \n",
    "    return df_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43bc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2412ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function decomposing a matrix X (normalized dataframe) into matrices U,S,Vt\n",
    "#left singular vector, singular values, right singular vector\n",
    "#it returns matrices \n",
    "#U left singular vector\n",
    "#s=array with singular values\n",
    "#Vt=right singular vector(transpose)\n",
    "#S=diagonal matrix with singular values along diagonal\n",
    "\n",
    "#X_stand=standardized dataframe  X_stand= (X-mean)/std of X\n",
    "#requires previously\n",
    "#from scipy.linalg import svd and \n",
    "#import numpy as np\n",
    "\n",
    "def fct_svd_decomposition(X_stand):\n",
    "    U, s, Vt = svd(X_stand,full_matrices=False)\n",
    "    S=np.diag(s)\n",
    "    return U,s,Vt,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9a1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5af04bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct veryfying the svd decomposition\n",
    "#t returns True/False depending whether the product \n",
    "#of U,s,Vt returns a matrix close to the initial matrix to decompose\n",
    "\n",
    "#val_initial_matrix_to_decompose= the initial standardized dataframe \n",
    "#val_component_S=diagonal matrix with singular values along the diagonal\n",
    "def fct_verify_svd_decomposition(\\\n",
    "val_initial_matrix_to_decompose,\n",
    "val_component_U,\n",
    "val_component_S,\n",
    "val_component_Vt):\n",
    "    \n",
    "    V=val_component_Vt.T\n",
    "    \n",
    "    return np.allclose(val_initial_matrix_to_decompose, \\\n",
    "                       val_component_U@val_component_S@V.T)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311be97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11cfdf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function projecting data into a lower dimension space\n",
    "#it returns the projected dataframe\n",
    "def fct_components_projection_to_lower_dim_space(\\\n",
    "val_component_U,\n",
    "val_component_S,\n",
    "val_new_data_dimension):\n",
    "    \n",
    "    feat_cols = ['PC'+str(i) for i in range(1,int(val_new_data_dimension+1))]\n",
    "    #print(\"feat_cols\",feat_cols)\n",
    "    \n",
    "    U_projected = val_component_U[:,:val_new_data_dimension]\n",
    "    S_projected = val_component_S[:val_new_data_dimension,:val_new_data_dimension]\n",
    "    \n",
    "    #a=U_projected@S_projected\n",
    "    #print(\"a\",a)\n",
    "    \n",
    "    df_projected = pd.DataFrame(U_projected@S_projected,columns=feat_cols)\n",
    "    \n",
    "    return df_projected\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811d7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8708993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returning  the explained variance and \n",
    "#the cummulative explained  variance\n",
    "#when considering each possible  number of features (data dimension)\n",
    "\n",
    "#val_array_singular_values=array with all the singular values\n",
    "\n",
    "def fct_variance_and_cum_explained_variance(val_array_singular_values):\n",
    "    \n",
    "    #the sum of all the singular values\n",
    "    sum_sing_vals=sum(val_array_singular_values)\n",
    "\n",
    "    # the explained variance of each singular value\n",
    "    var_explained = [(i/sum_sing_vals) for i in val_array_singular_values]\n",
    "    \n",
    "    # the cum explained variance\n",
    "    cum_var_explained=np.cumsum(var_explained)\n",
    "\n",
    "    return var_explained, cum_var_explained\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efadb56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c0c783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function computing and ploting \n",
    "#the  explained variance and the cumulative explained variance\n",
    "#it returns the explained variance, cumulative explained variance\n",
    "#and shows the plots\n",
    "\n",
    "def fct_compute_and_plot_sing_values_and_cum_variance_explained(\\\n",
    "val_array_singular_values,val_name_dataframe):\n",
    "    \n",
    "    #computation of the explained variance and \n",
    "    #the cumulative explained variance\n",
    "    var_explained, cum_var_explained=\\\n",
    "    fct_variance_and_cum_explained_variance(\\\n",
    "    val_array_singular_values=val_array_singular_values)\n",
    "    \n",
    "    \n",
    "    #plot the singular value ratio and the cumulative value\n",
    "    dim=val_array_singular_values.shape[0]+1\n",
    "    \n",
    "    x = np.arange(1,dim)\n",
    "    \n",
    "    sns.set(style=\"darkgrid\")\n",
    "    \n",
    "    res=sns.set(rc={'axes.facecolor':'lavender', 'figure.facecolor':'seashell'})\n",
    "\n",
    "    #fig, ax = plt.subplots(figsize=(15, 17))\n",
    "    fig, axes = plt.subplots(1,2,figsize=(12, 5))\n",
    "    \n",
    "    #title \n",
    "    fig.suptitle('Standardized Dataframe '+str(val_name_dataframe),color=\"purple\")\n",
    "    \n",
    "\n",
    "    sns.lineplot(\\\n",
    "    ax=axes[0],x=x,\\\n",
    "    y=var_explained,marker='o',color=\"purple\",linewidth=2)\n",
    "    \n",
    "    sns.lineplot(\\\n",
    "    ax=axes[1],x=x,\\\n",
    "    y=cum_var_explained,marker='*',color=\"magenta\",linewidth=2,\\\n",
    "    drawstyle='steps-pre')\n",
    "    \n",
    "    \n",
    "    axes[0].set_title(\"Explained Variance per Singular Value.\",c=\"purple\")\n",
    "    axes[0].set_xlabel('Id Singular Value',fontsize=12,c=\"purple\")\n",
    "    axes[0].set_ylabel('Explained Variance Ratio',fontsize=12,color=\"purple\")\n",
    "    axes[1].set_title(\"Cummulative Explained Variance.\",c=\"purple\")\n",
    "    axes[1].set_xlabel('Id Singular Value',fontsize=12,c=\"purple\")\n",
    "    axes[1].set_ylabel('Cumulative Explained Variance',fontsize=12,color=\"purple\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return var_explained, cum_var_explained\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e607d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c2cf96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function defining the space dimension (number of principal components,pc, to employ)\n",
    "#from the desired retained variable\n",
    "\n",
    "#val_desired_explained_variance= the desired explained variance\n",
    "def fct_define_nb_pcs_from_explained_variance(\\\n",
    "val_desired_explained_variance,\\\n",
    "val_cum_variance_explained):\n",
    "\n",
    "    return (val_cum_variance_explained<val_desired_explained_variance).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99065ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1f069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://vitalflux.com/pca-explained-variance-concept-python-example/\n",
    "\n",
    "#function returning the dataset  projection \n",
    "#into the directions capturing the greatest variance\n",
    "\n",
    "def fct_data_projected_in_lower_dimension_space(\\\n",
    "val_initial_standardized_matrix_to_decompose,\\\n",
    "val_desire_explained_variance,\\\n",
    "val_name_standardized_dataframe):\n",
    "\n",
    "    #matrix factorization\n",
    "    v_component_U,v_s,v_component_Vt,v_component_S=\\\n",
    "    fct_svd_decomposition(X_stand=val_initial_standardized_matrix_to_decompose)\n",
    "\n",
    "    #decomposition verification\n",
    "    rep_verification=fct_verify_svd_decomposition(\\\n",
    "    val_initial_matrix_to_decompose=val_initial_standardized_matrix_to_decompose,\n",
    "    val_component_U=v_component_U,\n",
    "    val_component_S=v_component_S,\n",
    "    val_component_Vt=v_component_Vt)\n",
    "    \n",
    "    if rep_verification==True:\n",
    "        print(\"response svd decomposition verification: \",rep_verification)\n",
    "    else:\n",
    "        print(\"response svd decomposition verification: \",rep_verification)\n",
    "        import sys\n",
    "        sys.exit()\n",
    "    \n",
    "    #function computing and ploting \n",
    "    #the  explained variance captured by each direction\n",
    "    #and the cumulative explained variance\n",
    "    #it returns the explained variance, cumulative explained variance\n",
    "    #and shows the plots\n",
    "\n",
    "    var_explained, cum_var_explained=\\\n",
    "    fct_compute_and_plot_sing_values_and_cum_variance_explained(\\\n",
    "    val_array_singular_values=v_s,\\\n",
    "    val_name_dataframe=val_name_standardized_dataframe)\n",
    "\n",
    "\n",
    "    #function defining the space dimension (number of principal components,pc, to employ)\n",
    "    #from the desired retained variable\n",
    "    val_new_space_dimension=fct_define_nb_pcs_from_explained_variance(\\\n",
    "    val_desired_explained_variance=val_desire_explained_variance,\\\n",
    "    val_cum_variance_explained=cum_var_explained)\n",
    "    \n",
    "    print(\"New (PC space) dimension: \", val_new_space_dimension)\n",
    "\n",
    "\n",
    "    #function projecting data into a lower dimension space\n",
    "    #it returns the projected dataframe\n",
    "    df_projected=fct_components_projection_to_lower_dim_space(\\\n",
    "    val_component_U=v_component_U,\n",
    "    val_component_S=v_component_S,\n",
    "    val_new_data_dimension=val_new_space_dimension)\n",
    "    \n",
    "    return df_projected\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550e9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634b75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returning a list with the inertias for a list \n",
    "#of values corresponding to  numbers of clusters\n",
    "\n",
    "#it alsa returns a dict : key=id nb clusters, value=kmeans object\n",
    "\n",
    "#val_type_init_kmeans_obj=\"random\" if KMeans algorithms\n",
    "#val_type_init_kmeans_obj='k-means++' if KMeans++ algorithms\n",
    "\n",
    "def fct_compute_inertia_for_list_of_values(\\\n",
    "val_name_dataframe,\\\n",
    "val_list_number_clusters,\\\n",
    "val_type_init_kmeans_obj,\\\n",
    "val_verbose_inertia):\n",
    "    li_inertias = []\n",
    "    \n",
    "    #di_kmeans_obj= dict, key=id nb clusters, value=kmeans object\n",
    "    di_kmeans_obj={}\n",
    "    \n",
    "    for i in val_list_number_clusters:\n",
    "        kmeans=\\\n",
    "        cluster.KMeans(n_clusters=i, init=val_type_init_kmeans_obj,n_init=\"auto\",\\\n",
    "                       verbose=val_verbose_inertia,\\\n",
    "                       random_state=42).fit(val_name_dataframe)\n",
    "        \n",
    "        li_inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        di_kmeans_obj[i]=kmeans\n",
    "    \n",
    "    return li_inertias, di_kmeans_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fbab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "144ba79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function ploting a list of values\n",
    "def fct_plot_li_vals(\\\n",
    "val_x,\\\n",
    "val_y,\\\n",
    "val_color,\\\n",
    "val_title_x_label=\"Number of clusters\",\\\n",
    "val_title_y_label=\"Inertia\",\n",
    "val_title=\"K-means random initialization, Inertia Score by n cluster centers\"):\n",
    "    \n",
    "    fig=plt.figure(figsize=(14,8))\n",
    "    plt.plot(val_x, val_y, '--o',color=val_color)\n",
    "    plt.xticks(list(range(1, 11)))#, list(range(1, 11)))\n",
    "    plt.title(val_title,color=val_color);\n",
    "    plt.xlabel(val_title_x_label,color=val_color)\n",
    "    plt.ylabel(val_title_y_label,color=val_color)\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578ba9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1e9a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct computing the clusters for a list of possible\n",
    "#number of centroids according to the kmean or keman++\n",
    "#and plot the inertia\n",
    "\n",
    "#it returns the dict of the kmeans obj where\n",
    "#key=nb of clusters, value= kmeans object\n",
    "def fct_define_clusters_and_plot_inertia(\\\n",
    "va_name_dataframe,\\\n",
    "va_list_number_clusters,\\\n",
    "va_type_init_kmeans_obj,\\\n",
    "va_color,\\\n",
    "va_verbose_inertia,\\\n",
    "va_title_x_label=\"Number of clusters\",\\\n",
    "va_title_y_label=\"Inertia\",\\\n",
    "va_title=\"K-means random initialization, Inertia Score by n cluster centers\"):\n",
    "\n",
    "    \n",
    "    #calculation of the list with the inertias for each desired\n",
    "    #number of clusters\n",
    "    li_inertias,di_kmeans_obj=fct_compute_inertia_for_list_of_values(\\\n",
    "    val_name_dataframe=va_name_dataframe,\\\n",
    "    val_list_number_clusters=va_list_number_clusters,\\\n",
    "    val_type_init_kmeans_obj=va_type_init_kmeans_obj,\\\n",
    "    val_verbose_inertia=va_verbose_inertia)\n",
    "    \n",
    "    #we ploth the value of the inertia versus the number of clusters\n",
    "    fct_plot_li_vals(\\\n",
    "    val_x=va_list_number_clusters,\\\n",
    "    val_y=li_inertias,\\\n",
    "    val_color=va_color,\\\n",
    "    val_title_x_label=va_title_x_label,\\\n",
    "    val_title_y_label=va_title_y_label,\n",
    "    val_title=va_title)\n",
    "    \n",
    "    return di_kmeans_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555ba44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec2ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/50297142/get-cluster-points-after-kmeans-in-a-list-format\n",
    "\n",
    "#function creating a dict: key= id cluster, value=id points in the cluster\n",
    "def fct_point_sets_per_cluster(val_cluster_object):\n",
    "    \n",
    "    #di_rep=dict, key=id cluster,\n",
    "    #value=arrays with the sample ids  in the cluster\n",
    "    \n",
    "    di_rep={}\n",
    "    \n",
    "    #di_kmeans_obj_1[4].cluster_centers_\n",
    "\n",
    "    #Labels of each point\n",
    "    #di_kmeans_obj_1[4].labels_\n",
    "\n",
    "    # !! Get the indices of the points for each corresponding cluster\n",
    "    di_rep = {i: np.where(val_cluster_object.labels_ == i)[0] \\\n",
    "          for i in range(val_cluster_object.n_clusters)}\n",
    "\n",
    "    return di_rep\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e44c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fb45f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct returning a dict  with the dataframes \n",
    "#containing the samples belonging to each cluster\n",
    "\n",
    "#val_di_points_per_cluster=dict, key=id cluster\n",
    "#value=array with the sample ids in the cluster\n",
    "\n",
    "#val_dataframe_with_all_samples= the dataframe with all the samples\n",
    "#the dataframe should be idexed by the sample id \n",
    "def fct_create_dict_dataframes_with_cluster_points_from_dict_pts_per_cluster(\\\n",
    "val_di_points_per_cluster,\\\n",
    "val_dataframe_with_all_samples):\n",
    "    \n",
    "    #key=id cluster, value=dataframe with the samples in the cluster\n",
    "    di_rep={}\n",
    "    \n",
    "    #for each cluster id\n",
    "    for i in val_di_points_per_cluster:\n",
    "        di_rep[i]=\\\n",
    "        val_dataframe_with_all_samples.iloc[list(val_di_points_per_cluster[i])]\n",
    "        \n",
    "    return di_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fd856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e293c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e43b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b3a9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct returning a dict  with the dataframes \n",
    "#containing the samples belonging to each cluster\n",
    "#from the cluster object\n",
    "\n",
    "def fct_create_dict_dataframes_with_cluster_points_from_cluster_object(\\\n",
    "val_cluster_object,\\\n",
    "val_df_with_all_samples):\n",
    "    \n",
    "    #di_rep_1=dict, key=id cluster,\n",
    "    #value=arrays with the sample ids  in the cluster\n",
    "    di_rep_1=fct_point_sets_per_cluster(val_cluster_object)\n",
    "    \n",
    "    #di_rep_2, dictionary\n",
    "    #key=id cluster, value=dataframe with the samples in the cluster\n",
    "    \n",
    "    di_rep_2=fct_create_dict_dataframes_with_cluster_points_from_dict_pts_per_cluster(\\\n",
    "    val_di_points_per_cluster=di_rep_1,\\\n",
    "    val_dataframe_with_all_samples=val_df_with_all_samples)\n",
    "    \n",
    "    return di_rep_2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c4fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c3ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct creating a dict with the dataframes \n",
    "#comprised of the samples belonging to each cluster\n",
    "#(when the number of desired clusters is already selected)\n",
    "#and saves the dictionary in memory\n",
    "\n",
    "#val_name_dict_pickled=the name of the dict with the dataframes\n",
    "#when saved in memory\n",
    "def fct_create_and_save_in_memory_dict_dfs_with_cluster_samples_from_cluster_object(\\\n",
    "val_cluster_obj,\\\n",
    "val_dataframe_with_all_samples,\\\n",
    "val_name_dict_dfs_pickled                                                                                    \n",
    "):\n",
    "    #create the dict, key=id  cluster value=datafrmae with the samples in the cluster\n",
    "    di=fct_create_dict_dataframes_with_cluster_points_from_cluster_object(\\\n",
    "    val_cluster_object=val_cluster_obj,\\\n",
    "    val_df_with_all_samples=val_dataframe_with_all_samples)\n",
    "    \n",
    "    #save the dictionary with the samples per cluster in memory\n",
    "    with open(val_name_dict_dfs_pickled, 'wb') as fp:\n",
    "        pickle.dump(di,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c5670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8c6db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct creating a dataframe with the coefficients of the best estimator\n",
    "#in an descending order\n",
    "\n",
    "#we consider that the gridsearchcv object has \n",
    "#a pipeline with the steps \n",
    "#[('poly_features', PolynomialFeatures(include_bias=False)),\n",
    "#('scaler', StandardScaler()),\n",
    "#('model', ...)]\n",
    "\n",
    "#val_gridsearch_fit_object: afit gridsearch object\n",
    "\n",
    "def fct_create_sorted_df_regarding_coef_decreasing_order(\\\n",
    "val_gridsearch_fit_object,\\\n",
    "val_step_model=\"model\"):\n",
    "    \n",
    "    #the names of the columns (features)\n",
    "    v_names_cols=\\\n",
    "    val_gridsearch_fit_object.best_estimator_.steps[0][1][0].get_feature_names_out() \n",
    "    \n",
    "    #the array with the coefficients\n",
    "    ar=val_gridsearch_fit_object.best_estimator_.steps[0][1][val_step_model].coef_\n",
    "    \n",
    "    #we reshape the array at so it wil be 1 row and ar.size columns\n",
    "    ar1=ar.reshape(1,ar.shape[0])\n",
    "    \n",
    "    #we create dataframe with the coefficients. per feature\n",
    "    df=pd.DataFrame(data=ar1,columns =v_names_cols)\n",
    "    \n",
    "    #we rearrnage the dataframe columns in an decreasing \n",
    "    #order accoriding to their value\n",
    "    df1=df[df.sum().sort_values(ascending=False).index]\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5c522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4adf45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function creating a dataframe with the non zero coefficients\n",
    "#per feature resulting from Lasso algorithm used as feature selector\n",
    "\n",
    "\n",
    "#val_name_feature_selector=the name the feature selector (steps)  in the pipleline\n",
    "#e.g. when pipeline is lasso_feature_selector_with_Ridge_model_pipe\n",
    "#then val_name_feature_selector ='feature_selector_l2\n",
    "\n",
    "\n",
    "\n",
    "def fct_create_df_with_Lasso_coefs_used_as_feature_selector(\\\n",
    "val_name_fit_grisearchcv_obj,\\\n",
    "val_name_feature_selector='feature_selector_l2'):\n",
    "    \n",
    "    #the coefficients of the selector Lasso passed in the Ridge model \n",
    "    #including the zeros\n",
    "    ar=val_name_fit_grisearchcv_obj.best_estimator_.named_steps['estim'].\\\n",
    "    named_steps[val_name_feature_selector].estimator_.coef_\n",
    "\n",
    "    ar1=ar.reshape(1,ar.shape[0])\n",
    "\n",
    "\n",
    "    #the columns createed by polynomeal features for the best dimension\n",
    "    v_names_cols=\\\n",
    "    val_name_fit_grisearchcv_obj.best_estimator_.steps[0][1][0].get_feature_names_out() \n",
    "    \n",
    "    #dataframe witht the coefs per feature, including the zero ones   \n",
    "    df=pd.DataFrame(data=ar1,columns =v_names_cols)\n",
    "\n",
    "    zero_cols =\\\n",
    "    [ col for col, is_zero in ((df == 0).sum() == df.shape[0]).items() if is_zero ]\n",
    "    df.drop(zero_cols, axis=1, inplace=True)\n",
    "\n",
    "    #(19) feature (coefficients ≠ 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428993b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13b6e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function creating barplot from a dataframe\n",
    "\n",
    "#val_df=dataframe to plot\n",
    "def fct_create_barplot(\\\n",
    "val_df_with_absolute_vals,\\\n",
    "val_title=\"RIDGE FEATURE IMPORTANCE\",\\\n",
    "val_xlabel=\"Feature\",\\\n",
    "val_ylabel=\"Ridge Coefficient\"):\n",
    "    \n",
    "    sns.set_style('darkgrid')\n",
    "\n",
    "    gfg=val_df_with_absolute_vals.plot(kind=\"bar\", figsize = (22, 14))\n",
    "    gfg.set_title(val_title,c=\"navy\",fontsize=22,weight='bold')\n",
    "    gfg.set_xlabel(val_xlabel,c=\"navy\",fontsize=22,weight='bold')\n",
    "    gfg.set_ylabel(val_ylabel,c=\"navy\",fontsize=22,weight='bold')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7b8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65dc1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function creating a dataframe with the non zero coefficients\n",
    "#per feature resulting from Lasso algorithm used as feature selector\n",
    "\n",
    "\n",
    "#val_name_feature_selector=the name the feature selector (steps)  in the pipleline\n",
    "#e.g. when pipeline is lasso_feature_selector_with_Ridge_model_pipe\n",
    "#then val_name_feature_selector ='feature_selector_l2\n",
    "\n",
    "\n",
    "\n",
    "def fct_create_df_with_Lasso_coefs_used_as_feature_selector(\\\n",
    "val_name_fit_grisearchcv_obj,\\\n",
    "val_name_feature_selector='feature_selector_l2',\\\n",
    "val_round=4):\n",
    "    \n",
    "    #the coefficients of the selector Lasso passed in the Ridge model \n",
    "    #including the zeros\n",
    "    ar=val_name_fit_grisearchcv_obj.best_estimator_.named_steps['estim'].\\\n",
    "    named_steps[val_name_feature_selector].estimator_.coef_\n",
    "\n",
    "    ar1=ar.reshape(1,ar.shape[0])\n",
    "\n",
    "\n",
    "    #the columns createed by polynomeal features for the best dimension\n",
    "    v_names_cols=\\\n",
    "    val_name_fit_grisearchcv_obj.best_estimator_.steps[0][1][0].get_feature_names_out() \n",
    "    \n",
    "    #dataframe with the coefs per feature, including the zero ones   \n",
    "    df=pd.DataFrame(data=ar1,columns =v_names_cols)\n",
    "\n",
    "    #zero_cols =\\\n",
    "    #[ col for col, is_zero in ((df == 0).sum() == df.shape[0]).items() if is_zero ]\n",
    "    \n",
    "    #we round all values of the lasso coefficients to 4 decimals\n",
    "    df=df.round(val_round)\n",
    "    \n",
    "    \n",
    "    # count the number of zeros in each column\n",
    "    zero_counts = (df == 0).sum(axis=0)\n",
    "\n",
    "    zero_cols = zero_counts[zero_counts != 0].index.tolist()\n",
    "    \n",
    "    df.drop(zero_cols, axis=1, inplace=True)\n",
    "    \n",
    "    #print(\"zero_cols\",zero_cols)\n",
    "    #print()\n",
    "\n",
    "    #print(\"df\", df)\n",
    "    #print(\"df.shape\", df.shape)\n",
    "    #(19) feature (coefficients ≠ 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc4936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "994688ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function creating a dataframe\n",
    "#with the Ridge (used as model) coefficients resulting from\n",
    "# the pipeline\n",
    "#Lasso feature selector and Ridge regression model\n",
    "\n",
    "\n",
    "#it returns a dataframe with the Ridge coefficients per column\n",
    "\n",
    "#val_fit_gridsearchcv_object=fit gridsearchcv object\n",
    "\n",
    "#val_df_with_coefs_from_lasso_selector=thh dataframe\n",
    "#with the coefficients of the selected features by the Lasso algorithm\n",
    "#used as selector\n",
    "#these coefficients are ≠ zero\n",
    "\n",
    "\n",
    "def fct_create_df_LR_or_Ridge_coefs_from_pipe_using_lasso_feature_selector(\\\n",
    "val_fit_gridsearchcv_object,\\\n",
    "val_df_with_coefs_from_lasso_selector):\n",
    "    \n",
    "    #the coefficients of the selector Lasso passed in the Ridge model \n",
    "    #including the zeros\n",
    "    #ar=\\\n",
    "    #val_fit_gridsearchcv_object.best_estimator_.named_steps['estim'].\\\n",
    "    #named_steps['feature_selector_l2']\\\n",
    "    #.estimator_.coef_\n",
    "    \n",
    "    coef_model=\\\n",
    "    val_fit_gridsearchcv_object.best_estimator_.named_steps[\"estim\"][\"model\"].coef_\n",
    "    \n",
    "\n",
    "    coef_model_reshaped=coef_model.reshape(1,coef_model.shape[0])\n",
    "    \n",
    "\n",
    "    #the columns createed by polynomeal features for the best dimension\n",
    "    #v_names_cols=\\\n",
    "    #val_fit_gridsearchcv_object.best_estimator_.steps[0][1][0].get_feature_names_out() \n",
    "    \n",
    "    \n",
    "    #dataframe witht the coefs per feature,  \n",
    "    df_model=\\\n",
    "    pd.DataFrame(data=coef_model_reshaped,columns=\\\n",
    "                 val_df_with_coefs_from_lasso_selector.columns)\n",
    "\n",
    "    \n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f89ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d320f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which treats and plots the resuts\n",
    "#of the pipeline\n",
    "#Lasso feature selector and Ridge regression model\n",
    "#that is the pipeline\n",
    "#lasso_feature_selector_with_Ridge_model_pipe\n",
    "\n",
    "#it returns a dataframe with the sorted absolute values \n",
    "#of the Ridge or LR coefs \n",
    "#in a decreasing order\n",
    "#when features are selected with \n",
    "#Lasso algo\n",
    "# and\n",
    "#a dateframe with the\n",
    "\n",
    "#val_round=the value to round the coefficients from Lasso selector\n",
    "#the vakue depends on the data\n",
    "#if too many deimals are zero then the  following algo that is Ridge may \n",
    "#see them as zeros and remove the variable\n",
    "#Consequently the number of features selected from Lasso will be\n",
    "#greater from the number of features utilized by Ridge and the \n",
    "#function fct_create_df_LR_or_Ridge_coefs_from_pipe_using_lasso_feature_selector\n",
    "#will create errors when it defines the dataframe with the Ridge coefficients\n",
    "def fct_plot_and_treat_LR_or_Ridge_results_when_lasso_feature_selector(\\\n",
    "val_fit_gridsearchcv_obj,\\\n",
    "val_name_feature_selector_lasso='feature_selector_l2',                                                                            \n",
    "val_title_fig=\"FEATURE IMPORTANCE\",\\\n",
    "val_xlabel_fig=\"Feature\",\\\n",
    "val_ylabel_fig=\"Coefficient\",\n",
    "val_round=4):\n",
    "    \n",
    "    #create the dataframe with the non zero coefficients\n",
    "    #resulting from Lasso whan used as feature selector\n",
    "    df_lasso_coefs_as_feature_selector=\\\n",
    "    fct_create_df_with_Lasso_coefs_used_as_feature_selector(\\\n",
    "    val_name_fit_grisearchcv_obj=val_fit_gridsearchcv_obj,\\\n",
    "    val_name_feature_selector=val_name_feature_selector_lasso,\\\n",
    "    val_round=val_round)\n",
    "    \n",
    "    #print(df_lasso_coefs_as_feature_selector)\n",
    "    \n",
    "    #create of the df with the Ridge coefficients\n",
    "    df=fct_create_df_LR_or_Ridge_coefs_from_pipe_using_lasso_feature_selector(\\\n",
    "    val_fit_gridsearchcv_object=val_fit_gridsearchcv_obj,\\\n",
    "    val_df_with_coefs_from_lasso_selector=df_lasso_coefs_as_feature_selector)                                                                               \n",
    "\n",
    "    \n",
    "    #plot the absolute values of the Ridge  coefficients as barplot\n",
    "    fct_create_barplot(\\\n",
    "    val_df_with_absolute_vals=df.abs(),\\\n",
    "    val_title=val_title_fig,\\\n",
    "    val_xlabel=val_xlabel_fig,\\\n",
    "    val_ylabel=val_ylabel_fig)\n",
    "                                                                            \n",
    "                                                                            \n",
    "                                                                            \n",
    "    #we rearrange the  absolute values of the dataframe columns \n",
    "    #(which are the Ridge feature coefficients)\n",
    "    #in an decreasing_order\n",
    "    \n",
    "    df1_abs_sorted=df.abs()[df.abs().sum().sort_values(ascending=False).index]\n",
    "    \n",
    "    #we also return the real values of the coefficients sorted \n",
    "    df1_sorted=df[df.sum().sort_values(ascending=False).index]\n",
    "    \n",
    "    return df1_abs_sorted, df1_sorted                                                                        \n",
    "                                                                           \n",
    "                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f86b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeb7c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct creating a dataframe with the coefficients of the best estimator\n",
    "#in an descending order for a pipeline without feature selection\n",
    "\n",
    "#it returns a sorted dataframe with the absolute values \n",
    "#of the (feature) coefficients in an decreasing order\n",
    "#and\n",
    "#a sorted dataframe with the values \n",
    "#of the (feature) coefficients in an decreasing order\n",
    "\n",
    "#we consider that the gridsearchcv object has \n",
    "#a pipeline with the steps \n",
    "#[('poly_features', PolynomialFeatures(include_bias=False)),\n",
    "#('scaler', StandardScaler()),\n",
    "#('model', ...)]\n",
    "\n",
    "#val_gridsearch_fit_object: afit gridsearch object\n",
    "\n",
    "def fct_create_sorted_df_regarding_model_coef_no_feature_selection_decreasing_order(\\\n",
    "val_gridsearch_fit_object,\\\n",
    "val_step_model=\"model\"):\n",
    "    \n",
    "    #the names of the columns (features)\n",
    "    v_names_cols=\\\n",
    "    val_gridsearch_fit_object.best_estimator_.steps[0][1][0].get_feature_names_out() \n",
    "    \n",
    "    #the array with the coefficients (dimension of the array (nb coeffic, 1))\n",
    "    ar=val_gridsearch_fit_object.best_estimator_.steps[0][1][val_step_model].coef_\n",
    "    \n",
    "    #we reshape the array at so it wil be 1 row and ar.size columns\n",
    "    ar1=ar.reshape(1,ar.shape[0])\n",
    "    \n",
    "    #we create dataframe with the coefficients. per feature\n",
    "    df=pd.DataFrame(data=ar1,columns =v_names_cols)\n",
    "    \n",
    "    #we rearrange the dataframe columns in an decreasing \n",
    "    #order accoriding to their value\n",
    "    df1_abs_sorted=df.abs()[df.abs().sum().sort_values(ascending=False).index]\n",
    "    \n",
    "    df1_sorted=df[df.sum().sort_values(ascending=False).index]\n",
    "    \n",
    "    return df1_abs_sorted, df1_sorted\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09005143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7c9507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct creating and ploting a dataframe with the  feature coefficients\n",
    "#when no feature selection is employed \n",
    "#it returns \n",
    "#it returns a sorted dataframe with the absolute values \n",
    "#of the (feature) coefficients in an decreasing order\n",
    "#and\n",
    "#a sorted dataframe with the values \n",
    "#of the (feature) coefficients in an decreasing order\n",
    "\n",
    "def fct_plot_and_treat_coefficients_model_without_feature_selection(\\\n",
    "val_gridsearchcv_fit_object,\\\n",
    "val_step_model=\"model\",\\\n",
    "val_title_fig=\"FEATURE IMPORTANCE\",\\\n",
    "val_xlabel_fig=\"Feature\",\\\n",
    "val_ylabel_fig=\"Ridge Coefficient\"):\n",
    "    \n",
    "    #create the dataframes with the absolute and real values of the \n",
    "    #coefficients of each feature in descending order\n",
    "    df_abs_sorted,df_sorted=\\\n",
    "    fct_create_sorted_df_regarding_model_coef_no_feature_selection_decreasing_order(\\\n",
    "    val_gridsearch_fit_object=val_gridsearchcv_fit_object,\\\n",
    "    val_step_model=\"model\")\n",
    "    \n",
    "    #barplots of the absolute values of the coefficients\n",
    "    fct_create_barplot(\\\n",
    "    val_df_with_absolute_vals=df_abs_sorted,\\\n",
    "    val_title=val_title_fig,\\\n",
    "    val_xlabel=val_xlabel_fig,\\\n",
    "    val_ylabel=val_ylabel_fig)\n",
    "    \n",
    "    return df_abs_sorted,df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70707a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c99edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it returns a dictionary\n",
    "#key=the number of samples\n",
    "#value=the number of clusters\n",
    "\n",
    "#and a dict, wuth the dbscan objects  per sample and epsilon\n",
    "#key=value of  sample, value= idt, key=value of epsilon, value=dbscan object\n",
    "\n",
    "#IT SHOULD BE ADDED A DICT WITH THE DATAFRAMES PER VALUE OF SAMPLE\n",
    "#EACH DATAFRAME WIL HAVE THE SAMPLES FOR EACH  CLUSTET CORRESPONDING\n",
    "#TO THE VALUE OF KEY EPSILON\n",
    "#THIS WILL BE USED TO FIND THE MOST IMPORTANT FEATURES OF EACH CLUSTER\n",
    "#CREATED BY DBSCAN JUST LIKE WE DID WITH THE OTHER UNSUPERVISED ALGOS\n",
    "#KMEANS AND KMANS++ (TO BE DONE)\n",
    "\n",
    "def fct_create_dbscan_clusters(\\\n",
    "val_array_epsilons,\\\n",
    "val_li_samples,\n",
    "val_dataframe\n",
    "):\n",
    "    \n",
    "    #di=dict\n",
    "    #key=nb samples\n",
    "    #value=[,...nb clusters for the ith epsilon,.....]\n",
    "    di={}\n",
    "    \n",
    "    \n",
    "    #di_1=dict\n",
    "    #key=id sample, value=dict, key=id epsilon, value=dbscan object\n",
    "    di_1={}\n",
    "    \n",
    "    #for each sample\n",
    "    for i in val_li_samples:\n",
    "        #a=round(i,2)\n",
    "        di[i]=[]\n",
    "        \n",
    "        di_1[i]={}\n",
    "        #for each epsilon\n",
    "        for j in val_array_epsilons:\n",
    "        \n",
    "            dbscan = cluster.DBSCAN(eps=j, min_samples=i).fit(val_dataframe)\n",
    "            \n",
    "            n_clusters = len(np.unique(dbscan.labels_))\n",
    "            \n",
    "            di[i].append(n_clusters)\n",
    "            \n",
    "            di_1[i][j]=dbscan\n",
    "            \n",
    "        \n",
    "            \n",
    "    return di, di_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e53a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43688484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_di=dict\n",
    "#key=nb samples\n",
    "#value=[..., nb clusters for the ith epsilon,.....]\n",
    "def fct_plot_clusters_versus_epsilon(val_di,val_array_epsilons):\n",
    "    \n",
    "    plt.figure(figsize=(14,7))\n",
    "    \n",
    "    for i in val_di.keys():\n",
    "    #print(i)\n",
    "        plt.plot(val_array_epsilons, val_di[i],label=\"nb samples: \"+str(i))\n",
    "        plt.xlabel('Epsilon')\n",
    "        plt.ylabel('Number of Clusters')\n",
    "        plt.title(\"How the Number of Clusters varies with parameter epsilon \")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e217608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecee3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fct_create_and_plot_dbscan_clusters(\\\n",
    "val_array_epsilons,\\\n",
    "val_li_samples,\n",
    "val_dataframe):\n",
    "    \n",
    "    #di=dict\n",
    "    #key=the number of samples\n",
    "    #value=the number of clusters\n",
    "    \n",
    "    #di_2=dict\n",
    "    #key=the number of samples\n",
    "    #value=the dbscan object\n",
    "    di,di_2=fct_create_dbscan_clusters(\\\n",
    "    val_array_epsilons=val_array_epsilons,\\\n",
    "    val_li_samples=val_li_samples,\n",
    "    val_dataframe=val_dataframe\n",
    "    )\n",
    "    \n",
    "    #we plot the number of clusters per value of parameter \\epsilon\n",
    "    fct_plot_clusters_versus_epsilon(val_di=di,\\\n",
    "                                    val_array_epsilons=val_array_epsilons)\n",
    "    \n",
    "    #di_dfs=fct_create_dict_dataframes_with_cluster_points_from_cluster_object(\\\n",
    "    #val_cluster_object,\\\n",
    "    #val_df_with_all_samples)\n",
    "    \n",
    "    return di,di_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc8f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87568279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118e019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8ba48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
